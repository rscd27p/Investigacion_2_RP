{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "relevant-screw",
   "metadata": {},
   "source": [
    "# Investigation 2 - Supervised Learning\n",
    "\n",
    "## Pattern Recognition MP6122 - Master of Science in Electronics - Emphasis on Embedded Systems\n",
    "\n",
    "## Costa Rica TEC\n",
    "\n",
    "Professor: MSc. Felipe Meza\n",
    "Student: Randy CÃ©spedes Deliyore - ID: 201054417\n",
    "\n",
    "This notebook would be used to compare the results of doing a bechmark over different supervised learning algorithms over the data set is named **COVID-19 Airline Flight Delays and Cancellations** and can be found [here](https://www.kaggle.com/akulbahl/covid19-airline-flight-delays-and-cancellations).\n",
    "\n",
    "The processing of this dataset is done in the jupyter notebook named [Investigacion2_Cespedes_Randy.ipynb](https://github.com/rscd27p/Investigacion_2_RP).\n",
    "\n",
    "The intial dataset was over 600 MB and was reduced to 35 MB.\n",
    "\n",
    "The dataset would be used to classify delayed flights for their main cause of delay. The classes are:\n",
    "\n",
    "CC: Carrier Delay.<br/>\n",
    "WD: Weather Delay.<br/>\n",
    "ND: NAS Delay.<br/>\n",
    "SD: Security Delay.<br/>\n",
    "LAD: Late Aircraft Delay.<br/>\n",
    "\n",
    "Since there are five different classes, this is considered a _multi-class_ classification problem. \n",
    "\n",
    "## Evaluation Metrics or Techniques\n",
    "\n",
    "Using the right metrics to evaluate an algorithms is key to evaluate their efectivity and have a common ground to compare multiple type of models. The information related to this metric was extracted from this two documents:\n",
    "\n",
    "$[1]$[20 Popular Machine Learning Metrics. Part 1: Classification & Regression Evaluation Metrics](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce) by [Shervin Minaee](https://sites.google.com/site/shervinminaee/home), who is a Machine Learning Researcher that holds a PhD in Electrical and Computer Engineering and Computer Science.\n",
    "\n",
    "$[2]$[Metrics to Evaluate your Machine Learning Algorithm](https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234) by [Aditya Mishra](https://medium.com/@adi_myth).\n",
    "\n",
    "$[3]$[Accuracy, Precision, Recall or F1?](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9) by [Koo Ping Shung](https://koopingshung.com/about/).\n",
    "\n",
    "\n",
    "### 1- Classification Accuracy (Used)\n",
    "\n",
    "Both articles agree that is a simple metric that allows to keep track of the **ratio of correct predictions**. This is represented by the equation below:\n",
    "\n",
    "$ Accurracy = \\frac{Number of Correct Predictions}{Total Number of Predictions Made} $\n",
    "\n",
    "Based on **$[1]$** there is a caveat when using it with samples that belong two different classes. For example, Lets asume that we have two classes:\n",
    "\n",
    "1-Class A.<br/>\n",
    "2-Class B.<br/>\n",
    "\n",
    "If we use 98% of elements of Class A and 2% of Class B in our training set, its possible to get a training accurracy of up to 98%. However, if we use 60% of elements from Class A, and 40% from Class B. We can get a value of up to 60% accuracy, which is a good result. However this does not mean that the system is going to recognize a 100% of the samples all the time.\n",
    "\n",
    "### 2- Logarithmic Loss (Used)\n",
    "\n",
    "The logaritmic loss ir _log loss_ is used to penalize missclassifications.This works particularly well with multi-class classification algorithms. When using this metric, the classifier must assign a probability for an element belonging to each class for all the samples. The _log loss_ is represented by the following equation:\n",
    "\n",
    "$ LogaritmicLoss = \\frac{-1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{ij} * log(p_{ij}) $ <br/><br/>\n",
    "$ N: Samples belonging to M classes $ <br/><br/>\n",
    "$ M: Classes $ <br/><br/>\n",
    "$ y_{ij} indicates the probability that sample i belongs to class j or not$ <br/><br/>\n",
    "$ p_{ij} indicates the probability that sample i belongs to class j$ <br/><br/>\n",
    "\n",
    "The log loss does not have an upper bound so it means and could have any value from zero to + infinity. The lower the value is to zero the higher is the accuracy.\n",
    "\n",
    "### 3- Confusion Metric (Used)\n",
    "\n",
    "A confusion matrix provides a summary of the performance of the whole model. It is easier to use an example to explain it. Lets assume there is a group of 1000 samples that could either be classified as **YES** or **NO**. (This example is adapted from $[2]$).\n",
    "\n",
    "| Number of Samples= 165 \t| Predicted NO \t| Predicted YES \t|\n",
    "|:----------------------:\t|:------------:\t|:-------------:\t|\n",
    "|        Actual NO       \t|      50      \t|       10      \t|\n",
    "|       Actual YES       \t|       5      \t|      100      \t|\n",
    "\n",
    "Iti s important to understand the four terms in the confusion matrix below:\n",
    "\n",
    "|   Number of Samples \t|  Predicted NO  \t|  Predicted YES \t|\n",
    "|:----------------------:\t|:--------------:\t|:--------------:\t|\n",
    "|        Actual NO       \t|  True Negative \t| False Positive \t|\n",
    "|       Actual YES       \t| False Negative \t|  True Positive \t|\n",
    "\n",
    "### 4- F1 Score (Used)\n",
    "\n",
    "**F1 Score** is a combination of two metrics named:\n",
    "\n",
    " - **Precision**: Based on $ [2] $ there are many situations in which the use of the _classification accurancy_ is not a good indicator of performance. For example, in cases where there are multi-class datasets and that there is imbalance, just like in the case of the dataset being used.The idea behind is to create a class-related metric, due to that it has to be done for each class independently and it is define by the following equation:\n",
    "\n",
    "$ Precision = \\frac{True\\_Positive}{True\\_Positive + False\\_Positive} $<br/><br/>\n",
    "\n",
    "The equation above uses the terms higlighed in the confusion matrix below:\n",
    "\n",
    "|   Number of Samples \t|  Predicted NO  \t|  Predicted YES \t|\n",
    "|:----------------------:\t|:--------------:\t|:--------------:\t|\n",
    "|        Actual NO       \t|  True Negative \t| <span style=\"color: red\">**False Positive**</span> \t|\n",
    "|       Actual YES       \t| False Negative \t|  <span style=\"color: red\">**True Positive**</span>.\t|\n",
    "\n",
    "\n",
    " - **Recall**: it is the number of correct positive results divided by the number of all the relevant samples $[1]$. It is define by the following equation:\n",
    " \n",
    "$ Recall = \\frac{True\\_Positive}{True\\_Positive + False\\_Positive} $\n",
    "\n",
    "The equation above uses the terms higlighed in the confusion matrix below:\n",
    "\n",
    "|   Number of Samples \t|  Predicted NO  \t|  Predicted YES \t|\n",
    "|:----------------------:\t|:--------------:\t|:--------------:\t|\n",
    "|        Actual NO       \t|  True Negative \t| False Positive \t|\n",
    "|       Actual YES       \t| <span style=\"color: blue\">**False Negative**</span> \t|  <span style=\"color: blue\">**True Positive**</span>.\t|\n",
    " \n",
    " <br/><br/>\n",
    "\n",
    "The equation for the F1 Score is:\n",
    "\n",
    "$ F1 = 2 * \\frac{Precision*Recall}{Precision+Recall} $\n",
    "\n",
    "**Important:** Precision, Recall and F1 are values between $[0:1]$ that could be represented by percentages by multipying them by a 100.\n",
    "\n",
    "### 5- Specificity or Specificity (Used)\n",
    "\n",
    "This metric uses the data from the:\n",
    "\n",
    "- **True Negative**.\n",
    "- **False Positive**.\n",
    "- **False Negative**.\n",
    "\n",
    "The equation is:\n",
    "\n",
    " $ Specificity = \\frac{True Negative}{True Negative + False Positive} $\n",
    "\n",
    "\n",
    "## Cross Validation and Grid Search\n",
    "\n",
    "$[4]$[Cross Validation and Grid Search for Model Selection in Python](https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/) by [Usman Malik](https://twitter.com/usman_malikk), PhD in artificial Intelligence.\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "In $[4]$ its explained how regularly data is divided into two subgroups:\n",
    "\n",
    "- **Training Datasets**.\n",
    "- **Test Datasets**.\n",
    "\n",
    "It is described how the acurracy obtained with the Training Dataset, used to train the model, could be very different from the one obtained with the Test Sets due to an invariance problems. A solution to this issue is to use K-Fold Cross-Validation. This method is used to divide the dataset into a $ **K** $ number of subdatasets. Then all the datasets except K-1 are used for training and the remaining dataset is used for testing.\n",
    "\n",
    "+Insert Image+ https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Machine learning models commonly have two kinds of parameters:\n",
    "\n",
    "- **Learned Parameters:** This parameters are learned by the machine learning model.\n",
    "- **Hyper Parameters**: The hyper parameters area the ones that are passed to learning model. The comon methodology based on $[4]$ is to randomly set the value of this and which values yield the best results. However, doing this randomly is ineficient and could take a long time. A Grid Search algorithm is one that could help find the best values for these hyper parameters.\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "The models reviewed in class were:\n",
    "\n",
    "- **Linear Regression**\n",
    "- **Linear Models for Multi-class classification**\n",
    "- **KNN for classification**\n",
    "- **Naive Bayes**\n",
    "- **Decision Trees**\n",
    "- **Random Forest**\n",
    "- **Kernel SVM**\n",
    "\n",
    "### Import General Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constant-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-double",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Import scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-thirty",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "It is important to divide the dataset into testing and training data. It was decided to use 20% of the data for training and 80% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "foster-reservoir",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>MKT_UNIQUE_CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY_NAME</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>...</th>\n",
       "      <th>ARR_DELAY_GROUP</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>DELAY_CAUSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>ONT</td>\n",
       "      <td>Ontario, CA</td>\n",
       "      <td>SFO</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>1810.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>LAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>ONT</td>\n",
       "      <td>Ontario, CA</td>\n",
       "      <td>SMF</td>\n",
       "      <td>Sacramento, CA</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>LAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>ORF</td>\n",
       "      <td>Norfolk, VA</td>\n",
       "      <td>BWI</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>LAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>PBI</td>\n",
       "      <td>West Palm Beach/Palm Beach, FL</td>\n",
       "      <td>ISP</td>\n",
       "      <td>Islip, NY</td>\n",
       "      <td>755.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>PDX</td>\n",
       "      <td>Portland, OR</td>\n",
       "      <td>DEN</td>\n",
       "      <td>Denver, CO</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>991.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  MONTH  DAY_OF_MONTH  DAY_OF_WEEK MKT_UNIQUE_CARRIER ORIGIN  \\\n",
       "0           0      1             1            3                 WN    ONT   \n",
       "1           7      1             1            3                 WN    ONT   \n",
       "2          14      1             1            3                 WN    ORF   \n",
       "3          24      1             1            3                 WN    PBI   \n",
       "4          29      1             1            3                 WN    PDX   \n",
       "\n",
       "                 ORIGIN_CITY_NAME DEST     DEST_CITY_NAME  CRS_DEP_TIME  ...  \\\n",
       "0                     Ontario, CA  SFO  San Francisco, CA        1810.0  ...   \n",
       "1                     Ontario, CA  SMF     Sacramento, CA        1505.0  ...   \n",
       "2                     Norfolk, VA  BWI      Baltimore, MD        1130.0  ...   \n",
       "3  West Palm Beach/Palm Beach, FL  ISP          Islip, NY         755.0  ...   \n",
       "4                    Portland, OR  DEN         Denver, CO        1310.0  ...   \n",
       "\n",
       "   ARR_DELAY_GROUP  ACTUAL_ELAPSED_TIME  AIR_TIME  DISTANCE  CARRIER_DELAY  \\\n",
       "0              4.0                122.0      74.0     363.0            8.0   \n",
       "1              3.0                 73.0      66.0     390.0            0.0   \n",
       "2              2.0                 47.0      36.0     159.0            6.0   \n",
       "3             11.0                152.0     139.0    1052.0          179.0   \n",
       "4              1.0                140.0     116.0     991.0           28.0   \n",
       "\n",
       "   WEATHER_DELAY  NAS_DELAY  SECURITY_DELAY  LATE_AIRCRAFT_DELAY  DELAY_CAUSE  \n",
       "0            0.0       27.0             0.0                 33.0          LAD  \n",
       "1            0.0        0.0             7.0                 40.0          LAD  \n",
       "2            0.0        0.0             0.0                 36.0          LAD  \n",
       "3            0.0        0.0             0.0                  0.0           CC  \n",
       "4            0.0        0.0             0.0                  0.0           CC  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "FlightsData = pd.read_csv(\"cleaned_FL_data.csv\")\n",
    "# Show a preview of the dataset\n",
    "\n",
    "FlightsData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-leeds",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "revised-thirty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>MKT_UNIQUE_CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>ORIGIN_CITY_NAME</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DEST_CITY_NAME</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>...</th>\n",
       "      <th>ARR_DELAY_GROUP</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>DELAY_CAUSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, MONTH, DAY_OF_MONTH, DAY_OF_WEEK, MKT_UNIQUE_CARRIER, ORIGIN, ORIGIN_CITY_NAME, DEST, DEST_CITY_NAME, CRS_DEP_TIME, DEP_TIME, DEP_DELAY, TAXI_OUT, WHEELS_OFF, WHEELS_ON, TAXI_IN, CRS_ARR_TIME, ARR_DELAY, ARR_DELAY_GROUP, ACTUAL_ELAPSED_TIME, AIR_TIME, DISTANCE, CARRIER_DELAY, WEATHER_DELAY, NAS_DELAY, SECURITY_DELAY, LATE_AIRCRAFT_DELAY, DELAY_CAUSE]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FlightsData[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-corrections",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-spectacular",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
